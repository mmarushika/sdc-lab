{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSYQyIr6FDGLjt97/7ojjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmarushika/sdc-lab/blob/main/LangChainTranslator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UPq4dLz--wia",
        "outputId": "4af57bec-b34c-4c06-9446-0a9d3af77694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Original: Good morning! How are you today?\n",
            "üåê Translated (French): Bonjour ! Comment allez-vous aujourd'hui ?\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install dependencies (Colab or Jupyter)\n",
        "!pip install -q langchain google-generativeai\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from typing import List, Optional, ClassVar\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Step 3: Set up Gemini API key (you said hardcoding is fine)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBzeETE_HhcD7zsf0bYY4uuoiiot6jjfvw\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Step 4: Custom Gemini 1.5 Flash wrapper for LangChain\n",
        "class GeminiFlashLLM(LLM):\n",
        "    model_name: ClassVar[str] = \"gemini-1.5-flash\"\n",
        "    temperature: ClassVar[float] = 0.3\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        try:\n",
        "            model = genai.GenerativeModel(self.model_name)\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"gemini-1.5-flash\"\n",
        "\n",
        "# Step 5: Define the prompt template for translation\n",
        "template = \"\"\"\n",
        "You are a professional translator. Translate the following text to {target_language}.\n",
        "\n",
        "Text: {input_text}\n",
        "\n",
        "Translation:\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Step 6: Create the chain\n",
        "llm = GeminiFlashLLM()\n",
        "translation_chain = prompt | llm\n",
        "\n",
        "# Step 7: Define a function to translate text\n",
        "def translate_text(input_text, target_language):\n",
        "    result = translation_chain.invoke({\n",
        "        \"input_text\": input_text,\n",
        "        \"target_language\": target_language\n",
        "    })\n",
        "    return result\n",
        "\n",
        "# Step 8: Example translation\n",
        "text_to_translate = \"Good morning! How are you today?\"\n",
        "target_lang = \"French\"\n",
        "\n",
        "translation = translate_text(text_to_translate, target_lang)\n",
        "print(f\"üî§ Original: {text_to_translate}\")\n",
        "print(f\"üåê Translated ({target_lang}): {translation}\")\n"
      ]
    }
  ]
}